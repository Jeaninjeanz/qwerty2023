#join tables
study_words <- left_join(study_words, total_words)
#stop words
#initial custom stopwords
my_stopwords <- tibble(word = c(as.character(1:22),
"research", "study", "information","systems","system","knowledge","data","based","paper","literature",
"technology", "model","findings","results","theory","reserved","©","rights","social","framework",
"study","theory"))
study_words <- study_words %>%
anti_join(stop_words, by = "word") %>%
anti_join(my_stopwords, by = "word")
#save and clear environment
saveRDS(study_words,"study_words.rds")
rm(list = ls(all.names = TRUE))
#load file
study_words <- readRDS("study_words.rds")
#perform calculations
study_tf_idf <- study_words %>%
bind_tf_idf(word, Title, n)
saveRDS(study_tf_idf,"study_tf_idf.rds")
#secondary custom stopwords
idf_stopwords <- study_tf_idf %>%
filter(idf < 5)
idf_stopwords <- tibble(word = unique(idf_stopwords$word))
#stop words
study_words <- study_words %>%
anti_join(idf_stopwords, by = "word")
#save and clear environment
saveRDS(study_words,"study_words.rds")
rm(list = ls(all.names = TRUE))
study_tf_idf <- readRDS("study_tf_idf.rds")
study_words <- readRDS("study_words.rds")
#tf-idf
study_tf_idf %>%
group_by(Year) %>%
slice_max(tf, n = 5) %>%
ungroup() %>%
ggplot(aes(tf, fct_reorder(word, tf))) +
geom_col(show.legend = FALSE) +
geom_text(aes(label = round(tf,2)),position = position_nudge(-0.1,0)) +
facet_wrap(~Year, scales = "free") +
labs(x = "Term Frequencies", y = NULL) +
labs(title = "Top 5 term frequencies in studies per year") +
theme(panel.grid.major.y = element_blank(),
panel.grid.minor.x = element_blank(),
panel.grid.minor.y = element_blank())
#w/o tf-idf
study_words %>%
slice_max(n, n = 45) %>%
ggplot(aes(n, word)) +
geom_col(show.legend = FALSE) +
geom_text(aes(label = round(n,2)),position = position_nudge(-1,0)) +
facet_wrap(~Year, scales = "free") +
labs(x = "Count", y = NULL) +
labs(title = "Top used words in studies per year")+
theme(panel.grid.major.y = element_blank(),
panel.grid.minor.x = element_blank(),
panel.grid.minor.y = element_blank())
#clear environment
rm(list = ls(all.names = TRUE))
#load
raw_data <-  read_csv("raw_data/IS_publications_2011_2020.csv")
head(raw_data)
#unnest
study_words <- raw_data %>%
unnest_tokens(word, Abstract) %>%
count(id, Title, word, Year, sort = TRUE)
#load
raw_data <-  read_csv("raw_data/IS_publications_2011_2020.csv")
head(raw_data)
#unnest
study_words <- raw_data %>%
unnest_tokens(word, Abstract) %>%
count(id, Title, word, Year, sort = TRUE)
#filtering
study_words <- study_words %>%
filter(!is.numeric(word)) %>%
filter(str_length(word) > 2)
#count words
total_words <- study_words %>%
group_by(Title) %>%
summarize(total = sum(n))
#join tables
study_words <- left_join(study_words, total_words)
#stop words
#initial custom stopwords
my_stopwords <- tibble(word = c(as.character(1:21),
"research", "study", "information","systems","system","knowledge","data","based","paper","literature",
"technology", "model","findings","results","theory","reserved","©","rights","social","framework",
"study"))
study_words <- study_words %>%
anti_join(stop_words, by = "word") %>%
anti_join(my_stopwords, by = "word")
#save and clear environment
saveRDS(study_words,"study_words.rds")
rm(list = ls(all.names = TRUE))
#load file
study_words <- readRDS("study_words.rds")
#perform calculations
study_tf_idf <- study_words %>%
bind_tf_idf(word, Title, n)
saveRDS(study_tf_idf,"study_tf_idf.rds")
#secondary custom stopwords
idf_stopwords <- study_tf_idf %>%
filter(idf < 5)
idf_stopwords <- tibble(word = unique(idf_stopwords$word))
#stop words
study_words <- study_words %>%
anti_join(idf_stopwords, by = "word")
#save and clear environment
saveRDS(study_words,"study_words.rds")
rm(list = ls(all.names = TRUE))
study_tf_idf <- readRDS("study_tf_idf.rds")
study_words <- readRDS("study_words.rds")
#tf-idf
study_tf_idf %>%                                                          #i have no idea what's causing the double print on some of the
group_by(Year) %>%                                                      #labels
slice_max(tf, n = 5) %>%
ungroup() %>%
ggplot(aes(tf, fct_reorder(word, tf))) +
geom_col(show.legend = FALSE) +
geom_text(aes(label = round(tf,2)),position = position_nudge(-0.1,0)) +
facet_wrap(~Year, scales = "free") +
labs(x = "Term Frequencies", y = NULL) +
labs(title = "Top 5 term frequencies in studies per year") +
theme(panel.grid.major.y = element_blank(),
panel.grid.minor.x = element_blank(),
panel.grid.minor.y = element_blank())
#w/o tf-idf
study_words %>%
slice_max(n, n = 45) %>%
ggplot(aes(n, word)) +
geom_col(show.legend = FALSE) +
geom_text(aes(label = round(n,2)),position = position_nudge(-1,0)) +
facet_wrap(~Year, scales = "free") +
labs(x = "Count", y = NULL) +
labs(title = "Top used words in studies per year")+
theme(panel.grid.major.y = element_blank(),
panel.grid.minor.x = element_blank(),
panel.grid.minor.y = element_blank())
#clear environment
rm(list = ls(all.names = TRUE))
#load file
study_words <- readRDS("study_words.rds")
#plotting
study_words %>%
filter(n > 5) %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 100, title.size = 1)
#save and clear environment
rm(list = ls(all.names = TRUE))
#load
study_words <- readRDS("study_words.rds")
#cast
topics_dtm <- study_words %>%
cast_dtm(id, word, n)
#LDA modeling
abstract_lda <- LDA(topics_dtm, k = 9, control = list(seed = 1234))
abstract_topics <- tidy(abstract_lda, matrix = "beta")
abstract_gamma <- tidy(abstract_lda, matrix = "gamma")
#save and clear environment
saveRDS(abstract_topics,"abstract_lda_beta.rds")
saveRDS(abstract_gamma,"abstract_lda_gamma.rds")
#rm(list = ls(all.names = TRUE))
#load
abstract_topics <- readRDS("abstract_lda_beta.rds")
abstract_gamma <- readRDS("abstract_lda_gamma.rds")
#filtering top 5 words per topic
top_terms <- abstract_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
#visualization
top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic)))+
geom_col(show.legend = FALSE)+
facet_wrap(~topic, scales = "free")+
scale_y_reordered() +
labs(title = "Top 10 terms per topic",
subtitle = "Analyzed using the per-topic-per-word probabilities") +
theme(panel.grid.major.y = element_blank(),
panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
panel.grid.minor.y = element_blank())
abstract_gamma %>%
ggplot(aes(factor(topic), gamma))+
geom_boxplot()+
facet_wrap(~topic) +
labs(x = "topic", y = expression(gamma))+
labs(title = "Gamma Distribution",
subtitle = "Analyzed using the per-document-per-topic probabilities") +
theme(panel.grid.major.y = element_blank(),
panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
panel.grid.minor.y = element_blank())
ggplot(abstract_gamma,aes(gamma, fill = as.factor(topic)))+
geom_histogram(alpha = 0.8, show.legend = FALSE) +
facet_wrap(~topic, ncol = 4) +
scale_y_log10() +
labs(title="Distribution of probability of each topic",
y = "Number of documents", x = expression(gamma))
#save and clear environment
#rm(list = ls(all.names = TRUE))
#load file
abstract_beta <- readRDS("abstract_lda_beta.rds")
sentiments <- get_sentiments("afinn")
#rename column
abstract_beta <- abstract_beta %>%
rename("word" = "term")
#joining
topic_tonality <- abstract_beta %>%
inner_join(get_sentiments("afinn"))
#plotting
topic_tonality %>%
group_by(topic) %>%
slice_max(beta, n = 100) %>%
summarize(sentiment = sum(value)) %>%
ungroup() %>%
ggplot(aes(topic, sentiment)) +
geom_col(show.legend = FALSE) +
scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9)) +
labs(title = "Tonality of topics",
subtitle = "Analyzed using top 100 per-topic-per-word probabilities AFINN values") +
theme(panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
panel.grid.minor.y = element_blank())
#clear environment
#rm(list = ls(all.names = TRUE))
library(tidyverse)
library(tidytext)
library(forcats)
library(wordcloud)
library(reshape2)
library(topicmodels)
library(ldatuning)
library(textdata)
library(widyr)
library(ggplot2)
library(igraph)
library(ggraph)
#load
raw_data <-  read_csv("raw_data/IS_publications_2011_2020.csv")
View(raw_data)
#load file
study_words <- readRDS("study_words.rds")
#perform calculations
study_tf_idf <- study_words %>%
bind_tf_idf(word, Title, n)
View(study_words)
study_tf_idf <- readRDS("study_tf_idf.rds")
View(study_tf_idf)
study_tf_idf %>%                                                          #i have no idea what's causing the double print on some of the
group_by(Year) %>%                                                      #labels
slice_max(tf, n = 5) %>%
ungroup() %>%
ggplot(aes(tf, fct_reorder(word, tf))) +
geom_col(show.legend = FALSE) +
geom_text(aes(label = round(tf,2)),position = position_nudge(-0.1,0)) +
facet_wrap(~Year, scales = "free") +
labs(x = "Term Frequencies", y = NULL) +
labs(title = "Top 5 term frequencies in studies per year") +
theme(panel.grid.major.y = element_blank(),
panel.grid.minor.x = element_blank(),
panel.grid.minor.y = element_blank())
abstract_lda_beta.rds
#load
abstract_topics <- readRDS("abstract_lda_beta.rds")
View(study_words)
View(study_words)
View(study_tf_idf)
View(abstract_topics)
#load
abstract_topics <- readRDS("abstract_lda_beta.rds")
abstract_gamma <- readRDS("abstract_lda_gamma.rds")
#filtering top 5 words per topic
top_terms <- abstract_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
#visualization
top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic)))+
geom_col(show.legend = FALSE)+
facet_wrap(~topic, scales = "free")+
scale_y_reordered() +
labs(title = "Top 10 terms per topic",
subtitle = "Analyzed using the per-topic-per-word probabilities") +
theme(panel.grid.major.y = element_blank(),
panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
panel.grid.minor.y = element_blank())
abstract_gamma %>%
ggplot(aes(factor(topic), gamma))+
geom_boxplot()+
facet_wrap(~topic) +
labs(x = "topic", y = expression(gamma))+
labs(title = "Gamma Distribution",
subtitle = "Analyzed using the per-document-per-topic probabilities") +
theme(panel.grid.major.y = element_blank(),
panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
panel.grid.minor.y = element_blank())
ggplot(abstract_gamma,aes(gamma, fill = as.factor(topic)))+
geom_histogram(alpha = 0.8, show.legend = FALSE) +
facet_wrap(~topic, ncol = 4) +
scale_y_log10() +
labs(title="Distribution of probability of each topic",
y = "Number of documents", x = expression(gamma))
#save and clear environment
#rm(list = ls(all.names = TRUE))
afinn
sentiments <- get_sentiments("afinn")
View(sentiments)
#load file
abstract_beta <- readRDS("abstract_lda_beta.rds")
sentiments <- get_sentiments("afinn")
#rename column
abstract_beta <- abstract_beta %>%
rename("word" = "term")
#joining
topic_tonality <- abstract_beta %>%
inner_join(get_sentiments("afinn"))
#plotting
topic_tonality %>%
group_by(topic) %>%
slice_max(beta, n = 100) %>%
summarize(sentiment = sum(value)) %>%
ungroup() %>%
ggplot(aes(topic, sentiment)) +
geom_col(show.legend = FALSE) +
scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9)) +
labs(title = "Tonality of topics",
subtitle = "Analyzed using top 100 per-topic-per-word probabilities AFINN values") +
theme(panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
panel.grid.minor.y = element_blank())
#clear environment
#rm(list = ls(all.names = TRUE))
library(tidyverse)
library(tidytext)
library(forcats)
library(wordcloud)
library(reshape2)
library(topicmodels)
library(ldatuning)
library(textdata)
library(widyr)
library(ggplot2)
library(igraph)
library(ggraph)
#load
raw_data <-  read_csv("raw_data/IS_publications_2011_2020.csv")
#unnest
author_words <- raw_data %>%
unnest_tokens(word, Authors) %>%
count(word, id, Year, sort = TRUE)
#filter
author_words <- author_words %>%
filter(str_length(word) > 1) %>%
filter(!grepl("[a-z]\\.[a-z]",word)) %>%
filter(!grepl("[a-z]\\.[a-z]\\.[a-z]",word))
#pairwise
author_pairs <- author_words %>%
pairwise_count(word, id, sort = TRUE, upper = FALSE)
author_pairs %>%
filter(n >= 10) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
geom_node_point(size = 5) +
geom_node_text(aes(label = name), repel = TRUE,
point.padding = unit(0.2, "lines")) +
labs(title="Co-Authors",
subtitle="Those that authored more than 10 items together")
theme_void()
#save and clear environment
rm(list = ls(all.names = TRUE))
#load
raw_data <-  read_csv("raw_data/IS_publications_2011_2020.csv")
#unnest
author_words <- raw_data %>%
unnest_tokens(word, Authors) %>%
count(word, id, Year, sort = TRUE)
#filter
author_words <- author_words %>%
filter(str_length(word) > 1) %>%
filter(!grepl("[a-z]\\.[a-z]",word)) %>%
filter(!grepl("[a-z]\\.[a-z]\\.[a-z]",word))
#pairwise
author_pairs <- author_words %>%
pairwise_count(word, id, sort = TRUE, upper = FALSE)
View(raw_data)
View(author_words)
View(raw_data)
View(author_pairs)
library(tidyverse)
library(tidytext)
library(forcats)
library(wordcloud)
library(reshape2)
library(topicmodels)
library(ldatuning)
library(textdata)
library(widyr)
library(ggplot2)
library(igraph)
library(ggraph)
aw_data <-  read_csv("raw_dat
#load
raw_data <-  read_csv("raw_data/IS_publications_2011_2020.csv")
#load
raw_data <-  read_csv("raw_data/IS_publications_2011_2020.csv")
View(raw_data)
#unnest
author_words <- raw_data %>%
unnest_tokens(word, Authors) %>%
count(word, id, Year, sort = TRUE)
View(author_words)
author_words <- author_words %>%
filter(str_length(word) > 1) %>%
author_words <- author_words %>%
filter(str_length(word) > 1) %>%
View(author_words)
ds <- author_words %>%
filter(str_length(word) > 1)
View(ds)
author_words <- author_words %>%
filter(str_length(word) > 1) %>%
filter(!grepl("[a-z]\\.[a-z]",word)) %>%
filter(!grepl("[a-z]\\.[a-z]\\.[a-z]",word))
x <- %>%
x <- 1
x <- x %>%
x+2 %>%
x/3
library(tidyverse)
x <- x %>%
x+2 %>%
x/3
x <- x %>%
x+2 %>%
x/3
library(caret)
library(tidyverse)
# Load the dataset into R
data <- read.csv("riskData.rds")
gc()
# Load the dataset into R
data <- readRDS("riskData.rds")
# Set the seed for reproducibility
set.seed(123)
# Create a training and testing split (80/20)
trainIndex <- createDataPartition(data$riskClass, p = 0.8, list = FALSE)
trainData <- data[trainIndex,]
testData <- data[-trainIndex,]
# Specify the model using the train function
model <- train(riskClass ~ ., data = trainData, method = "rf")
# Load the dataset into R
data <- readRDS("riskData.rds")
View(data)
data <- data %>%
filter(date < 2014-06-1)
library(caret)
library(tidyverse)
# Load the dataset into R
data <- readRDS("riskData.rds")
data <- data %>%
filter(date < 2014-06-1)
View(data)
# Load the dataset into R
data <- readRDS("riskData.rds")
data <- data %>%
filter(date < "2014-06-1")
# Load the dataset into R
data <- readRDS("riskData.rds")
data <- data %>%
filter(date < "2014-01-1")
data <- data %>%
filter(date < "2013-09-1")
data <- data %>%
filter(date < "2013-08-1")
# Load the dataset into R
data <- readRDS("riskData.rds")
data <- data %>%
filter(date < "2013-08-15")
# Set the seed for reproducibility
set.seed(123)
# Create a training and testing split (80/20)
trainIndex <- createDataPartition(data$riskClass, p = 0.8, list = FALSE)
trainData <- data[trainIndex,]
testData <- data[-trainIndex,]
# Specify the model using the train function
model <- train(riskClass ~ ., data = trainData, method = "rf")
setwd("~/Desktop/qwerty2023")
raw_data <- read.csv("data/raw/MorrisonData.csv")
View(raw_data)
setwd("~/Desktop/qwerty2023")
raw_data <- read.csv("data/raw/MorrisonData.csv")
head(raw_data[0,6]
head(raw_data[0,6])
raw_data[0,6]
raw_data[,6]
raw_data[,0-6]
raw_data[,0-5]
View(raw_data)
